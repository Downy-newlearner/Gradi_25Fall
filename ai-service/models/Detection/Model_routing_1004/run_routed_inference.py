#!/usr/bin/env python3
"""
ë¼ìš°íŒ… ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
YOLOv8n (í° ê°ì²´)ê³¼ YOLOv8s (ì‘ì€ ê°ì²´) ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ê° í´ë˜ìŠ¤ë³„ë¡œ ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ë°”ìš´ë”©ë°•ìŠ¤ë§Œ ì„ íƒí•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple
from ultralytics import YOLO
from datetime import datetime

# í´ë˜ìŠ¤ ë¼ìš°íŒ… ì •ì˜
SMALL_CLASSES = {"page_number", "problem_number", "answer_1", "answer_2"}
LARGE_CLASSES = {"korean_content", "english_content", "section", "answer_option"}

# í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ ì •ì˜
CLASS_COLORS = {
    # í° ê°ì²´ í´ë˜ìŠ¤
    'answer_option': (0, 255, 0),      # ë…¹ìƒ‰
    'english_content': (255, 0, 0),    # ë¹¨ê°„ìƒ‰
    'korean_content': (0, 0, 255),     # íŒŒë€ìƒ‰
    'section': (255, 255, 0),          # ë…¸ë€ìƒ‰
    
    # ì‘ì€ ê°ì²´ í´ë˜ìŠ¤
    'page_number': (255, 0, 255),      # ë§ˆì  íƒ€
    'problem_number': (0, 255, 255),   # ì‹œì•ˆ
    'answer_1': (255, 165, 0),         # ì˜¤ë Œì§€
    'answer_2': (128, 0, 128)          # ë³´ë¼
}

class RoutedInference:
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.small_model_dir = self.base_dir / "yolov8s_imgsz_2048"
        self.large_model_dir = self.base_dir / "yolov8n_imgsz_1280"
        
        # ëª¨ë¸ ê²½ë¡œ
        self.small_model_path = self.small_model_dir / "train" / "weights" / "best.pt"
        self.large_model_path = self.large_model_dir / "train" / "weights" / "best.pt"
        
        # ëª¨ë¸ ë¡œë“œ
        self.small_model = self._load_model(self.small_model_path, "yolov8s.pt")
        self.large_model = self._load_model(self.large_model_path, "yolov8n.pt")
        
        # ì¶”ë¡  íŒŒë¼ë¯¸í„°
        self.small_conf = 0.22
        self.large_conf = 0.12
        
        print(f"âœ… Small model loaded: {self.small_model_path}")
        print(f"âœ… Large model loaded: {self.large_model_path}")
    
    def _load_model(self, model_path: Path, fallback: str):
        """ëª¨ë¸ ë¡œë“œ"""
        if model_path.exists():
            return YOLO(str(model_path))
        else:
            print(f"âš ï¸ Model not found: {model_path}, using {fallback}")
            return YOLO(fallback)
    
    def get_all_detections(self, results, class_names: List[str]) -> List[Dict]:
        """
        ëª¨ë“  ê²€ì¶œ ê²°ê³¼ë¥¼ ë°˜í™˜ (í´ë˜ìŠ¤ë³„ ìµœê³  ì‹ ë¢°ë„ ì„ íƒ ì œê±°)
        """
        all_detections = []
        
        if results.boxes is not None:
            for box in results.boxes:
                cls_id = int(box.cls[0])
                confidence = float(box.conf[0])
                xyxy = box.xyxy[0].tolist()
                
                if cls_id < len(class_names):
                    class_name = class_names[cls_id]
                    
                    all_detections.append({
                        'class_name': class_name,
                        'class_id': cls_id,
                        'confidence': confidence,
                        'bbox': xyxy
                    })
        
        return all_detections
    
    def route_infer_single_image(self, image_path: str) -> Dict:
        """
        ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•´ ë¼ìš°íŒ… ì¶”ë¡  ìˆ˜í–‰
        """
        # ì‘ì€ ê°ì²´ ëª¨ë¸ ì¶”ë¡  (YOLOv8s @ 2048)
        small_results = self.small_model(str(image_path), imgsz=2048, conf=self.small_conf, verbose=False)[0]
        small_class_names = ['page_number', 'problem_number', 'answer_1', 'answer_2']
        small_detections = self.get_all_detections(small_results, small_class_names)
        
        # í° ê°ì²´ ëª¨ë¸ ì¶”ë¡  (YOLOv8n @ 1280)
        large_results = self.large_model(str(image_path), imgsz=1280, conf=self.large_conf, verbose=False)[0]
        large_class_names = ['answer_option', 'english_content', 'korean_content', 'section']
        large_detections = self.get_all_detections(large_results, large_class_names)
        
        # ê²°ê³¼ ë³‘í•©
        all_detections = small_detections + large_detections
        
        return {
            'image_path': str(image_path),
            'detections': all_detections,
            'small_detections': small_detections,
            'large_detections': large_detections,
            'timestamp': datetime.now().isoformat()
        }
    
    def visualize_detections(self, image_path: str, detections: List[Dict], output_path: str):
        """
        ê²€ì¶œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ ì €ì¥
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(image_path))
        if image is None:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            return
        
        # ê° ê²€ì¶œ ê²°ê³¼ ê·¸ë¦¬ê¸°
        for det in detections:
            class_name = det['class_name']
            confidence = det['confidence']
            bbox = det['bbox']
            color = CLASS_COLORS.get(class_name, (255, 255, 255))
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ
            x1, y1, x2, y2 = map(int, bbox)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(image, (x1, y1), (x2, y2), color, 3)
            
            # ë¼ë²¨ í…ìŠ¤íŠ¸
            label = f"{class_name}: {confidence:.3f}"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
            (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
            cv2.rectangle(image, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)
            
            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # ë²”ë¡€ ì¶”ê°€
        self.add_legend(image)
        
        # ì´ë¯¸ì§€ ì €ì¥
        cv2.imwrite(str(output_path), image)
    
    def add_legend(self, image):
        """ë²”ë¡€ ì¶”ê°€"""
        legend_y = 30
        all_classes = list(CLASS_COLORS.keys())
        
        for i, class_name in enumerate(all_classes):
            color = CLASS_COLORS[class_name]
            # ìƒ‰ìƒ ë°•ìŠ¤
            cv2.rectangle(image, (10, legend_y + i * 30), (30, legend_y + i * 30 + 25), color, -1)
            # í´ë˜ìŠ¤ ì´ë¦„
            cv2.putText(image, class_name, (35, legend_y + i * 30 + 18), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
    
    def process_test_images(self, test_data_root: str, output_dir: str):
        """
        í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ ë¼ìš°íŒ… ì¶”ë¡  ìˆ˜í–‰
        """
        test_data_path = Path(test_data_root)
        output_path = Path(output_dir)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        (output_path / "images").mkdir(parents=True, exist_ok=True)
        (output_path / "annotations").mkdir(parents=True, exist_ok=True)
        
        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        test_images_dir = test_data_path / "test" / "images"
        if not test_images_dir.exists():
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_images_dir}")
            return
        
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        image_files = []
        for ext in image_extensions:
            image_files.extend(test_images_dir.glob(f"*{ext}"))
            image_files.extend(test_images_dir.glob(f"*{ext.upper()}"))
        
        image_files = sorted(image_files)
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ {len(image_files)}ê°œ ë°œê²¬")
        
        # ê²°ê³¼ ì €ì¥ìš©
        all_results = []
        total_detections = 0
        class_counts = {name: 0 for name in CLASS_COLORS.keys()}
        total_confidence = 0
        
        print(f"ğŸ” {len(image_files)}ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ ë¼ìš°íŒ… ì¶”ë¡  ì‹œì‘...")
        
        for i, image_path in enumerate(image_files):
            print(f"ì²˜ë¦¬ ì¤‘: {i+1}/{len(image_files)} - {image_path.name}")
            
            # ë¼ìš°íŒ… ì¶”ë¡  ìˆ˜í–‰
            result = self.route_infer_single_image(str(image_path))
            all_results.append(result)
            
            # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥
            output_image_path = output_path / "images" / f"routed_result_{image_path.stem}.jpg"
            self.visualize_detections(str(image_path), result['detections'], str(output_image_path))
            
            # JSON ì €ì¥
            output_json_path = output_path / "annotations" / f"routed_result_{image_path.stem}.json"
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            for det in result['detections']:
                total_detections += 1
                class_name = det['class_name']
                if class_name in class_counts:
                    class_counts[class_name] += 1
                total_confidence += det['confidence']
        
        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        avg_confidence = total_confidence / total_detections if total_detections > 0 else 0
        summary = {
            "total_images": len(image_files),
            "processed_images": len(image_files),
            "total_detections": total_detections,
            "average_detections_per_image": total_detections / len(image_files) if image_files else 0,
            "class_counts": class_counts,
            "average_confidence": avg_confidence,
            "routing_strategy": {
                "small_objects": list(SMALL_CLASSES),
                "large_objects": list(LARGE_CLASSES),
                "small_model_conf": self.small_conf,
                "large_model_conf": self.large_conf
            }
        }
        
        summary_path = output_path / "routing_results_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š ë¼ìš°íŒ… ì¶”ë¡  ê²°ê³¼ ìš”ì•½:")
        print(f"  - ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {summary['processed_images']}/{summary['total_images']}")
        print(f"  - ì´ ê²€ì¶œ ìˆ˜: {summary['total_detections']}")
        print(f"  - ì´ë¯¸ì§€ë‹¹ í‰ê·  ê²€ì¶œ ìˆ˜: {summary['average_detections_per_image']:.1f}")
        print(f"  - í‰ê·  ì‹ ë¢°ë„: {summary['average_confidence']:.3f}")
        print("\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ê²€ì¶œ ìˆ˜:")
        for class_name, count in summary['class_counts'].items():
            print(f"  - {class_name}: {count}")
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"  - ì‹œê°í™” ì´ë¯¸ì§€: {output_path}/images/")
        print(f"  - ê²€ì¶œ ë°ì´í„°: {output_path}/annotations/")
        print(f"  - ê²°ê³¼ ìš”ì•½: {output_path}/routing_results_summary.json")
        
        return summary


def main():
    # ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent
    test_data_root = current_dir.parent.parent / "Data" / "english_large4_251004"  # í…ŒìŠ¤íŠ¸ìš©
    output_dir = current_dir / "routed_inference_results"
    
    print("ğŸš€ ë¼ìš°íŒ… ì¶”ë¡  ì‹œì‘")
    print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_data_root}")
    print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print("=" * 60)
    
    try:
        # ë¼ìš°íŒ… ì¶”ë¡  ì‹¤í–‰
        router = RoutedInference(str(current_dir))
        results = router.process_test_images(str(test_data_root), str(output_dir))
        
        print("âœ… ë¼ìš°íŒ… ì¶”ë¡  ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())